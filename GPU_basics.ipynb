{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbabe606",
   "metadata": {},
   "source": [
    "### GPU\n",
    "GPU(Graphics Processing Unit) - устройство, спроектированное специально для того, чтобы обрабатывать 3d изображения.\n",
    "Отличие этих вычислений от того, что мы делали раньше заключается в том, что для того, чтобы отобразить картинку, нужно сделать большое количество относительно простых операций с большим количеством объектов(например, рассчитать для каждой точки на плоскости, видна ли она в данный момент времени). Для этого используется большое количество простых ядер, работающих на меньшей частоте, чем те, что используются в процессорах.\n",
    "\n",
    "В определенных задачах это может давать серьёзный прирост по производительности, в других же, наоборот эффективней использовать процессор.\n",
    "\n",
    "### Задачи для GPU (параллельные, однородные вычисления):\n",
    "- Матричные операции\n",
    "- Обучение нейронных сетей\n",
    "- Обработка изображений\n",
    "- Симуляции физики (расчёт взаимодействия множества частиц)\n",
    "\n",
    "### Задачи для CPU (последовательные, сложная логика):\n",
    "- Парсинг и обработка текста\n",
    "- Работа с базами данных\n",
    "- Компиляция кода (сложный анализ, оптимизации)\n",
    "- Сортировка и поиск в небольших данных\n",
    "- Рекурсивные алгоритмы (обход деревьев, графов)\n",
    "\n",
    "\n",
    "### Итог:\n",
    "- GPU: много простых одинаковых операций над большими данными\n",
    "- CPU: сложная логика, ветвления, последовательные зависимости\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33218006",
   "metadata": {},
   "source": [
    "## История\n",
    "Изначально подобные схемы появились как отрисовщики изображения. Они ходили по памяти компьютера, куда процессор клал данные, и отрисовывали изображение на экран. Это было сделано для того, чтобы CPU занимался последовательными расчётами, не прерываясь на относительно простые, но частые задачи отрисовки кадров.\n",
    "\n",
    "Затем, первые GPU научились выполнять графические примитивы: отрисовка линий, заливка(благодаря специальным командам, оптимизированным для этих задач, ускоряется выполнение: там, где процессор последовательно отрисовывал каждый пиксель, GPU может сразу взять 100 и выполнить простую операцию для них). Появилась возможность выполнять сторонние вычисления(не только графические). \n",
    "\n",
    "Далее все шло к тому, чтобы сделать GPU как можно более гибкими: они научились выполнять циклы, операции для чисел с плавающей точкой.\n",
    "\n",
    "Поэтому можно сделать следующий вывод: GPU -- много-много медленных CPU, к которым мы можем обращаться одновременно(продвинутая многопоточность), однако есть некоторые ограничения, связанные с специализацией данных схем."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddd61c6",
   "metadata": {},
   "source": [
    "Сейчас самой популярной платформой является Nvidia CUDA(Compute Unified Device Architecture) - платформа для параллельных вычислений, под неё оптимизированно большое количество библиотек, поэтому давайте рассмотрим именно её."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52f7840",
   "metadata": {},
   "source": [
    "Более детельно посмотрим на устройство Nvidia A100:\n",
    "\n",
    "<img src=\"a100_sm.png\" width=\"500\" alt=\"description\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e872ef",
   "metadata": {},
   "source": [
    "Чип разбит на SM(Streaming Multiprocessor) - вычислительные блоки, каждый из который отвечает за выполнение потоков на выделенных ядрах.\n",
    "Ядра делятся на несколько типов:\n",
    "- CUDA - выполняют арифметические операции с int32, float32/64 \n",
    "- Tensor Core - специализированные ядра для матричных операций\n",
    "- RT-cores - ядра, для трассировки лучей(ускоренные проверки с лучами и т.п.)\n",
    "\n",
    "Когда поступает какая-то задача, то она разбивается на блоки потоков(каждый блок идёт на отдельный SM), эти блоки разбиваются на warp-ы -- 32 потока.\n",
    "Уже warp-ы исполняются. Причём задачи в одном warp -- одинаковые, т.е. мы можем сложить 32 int-a.\n",
    "\n",
    "Warp Scheduler отслеживает то, какие warp-ы готовы к исполнению и отправляет их к Dispatch Unit.(warp-у нужно загрузить из более медленной памяти какие-то данные -- он простаивает, если же есть другой уже готовый warp, то мы переключаемся на него)\n",
    "Dispatch Unit уже определяет, где и на каких ядрах будет выполняться данный warp. Так же он обращается к L0 кэшу за командой для текущего warp-a.\n",
    "\n",
    "По мере вычисления одного warp-а мы можем обращаться к register file - наиболее быстрой памяти.\n",
    "\n",
    "\n",
    "Иерархия памяти:\n",
    "| Тип памяти | Описание | Время обращения, тактов |\n",
    "|------------|----------|------------------------|\n",
    "| DRAM | память компьютера | 2000+ |\n",
    "| Global memory(видео память) | память, общая для всего GPU, её может быть порядка нескольких гигабайт | 200-400 |\n",
    "| L2 | служит для обмена L1 с глобальной памятью | 100-200 |\n",
    "| Shared memory(L1) | общая для блока потоков | 20-30 |\n",
    "| Register File | память, выделенная под выполнение конкретного warp | 1-2 |\n",
    "| L0 | хранилище команд | 1-2 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545cd461",
   "metadata": {},
   "source": [
    "## Numba CUDA - базовое руководство\n",
    "\n",
    "Для работы с CUDA можно использовать разные библиотеки. Давайте посмотрим на numba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b4c2f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA devices\n",
      "id 0             b'Tesla T4'                              [SUPPORTED]\n",
      "                      Compute Capability: 7.5\n",
      "                           PCI Device ID: 4\n",
      "                              PCI Bus ID: 0\n",
      "                                    UUID: GPU-517bb309-2f01-17b3-2703-6fa5306ee38c\n",
      "                                Watchdog: Disabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "Summary:\n",
      "\t1/1 devices are supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "cuda.detect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340ee75e",
   "metadata": {},
   "source": [
    "### Простое сложение векторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95387105",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def add_kernel(x, y, out):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < x.size:\n",
    "        out[idx] = x[idx] + y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a257cbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/cudadrv/devicearray.py:937: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "n = 1000000\n",
    "x = np.ones(n)\n",
    "y = np.ones(n) * 2\n",
    "out = np.zeros(n)\n",
    "\n",
    "threads_per_block = 256\n",
    "blocks_per_grid = (n + threads_per_block - 1) // threads_per_block\n",
    "\n",
    "add_kernel[blocks_per_grid, threads_per_block](x, y, out)\n",
    "print(out[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e37136",
   "metadata": {},
   "source": [
    "### Как стоит писать код внутри функции\n",
    "\n",
    "1. Каждый поток должен работать с одним элементом данных\n",
    "2. Избегайте ветвлений (if/else)\n",
    "3. Используйте локальные переменные вместо глобальной памяти\n",
    "4. Минимизируйте обращения к глобальной памяти\n",
    "5. Используйте shared memory для кэширования данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fcbc26",
   "metadata": {},
   "source": [
    "### Определение позиции внутри сетки потоков\n",
    "\n",
    "Для того, чтобы понимать, с каким элементом массива будет работать данная функция, можно привязаться к сетке потоков.\n",
    "Чтобы определить своё положение в сетке используем следующие функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfedeee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def test_func(out):\n",
    "    idx = cuda.grid(1)\n",
    "    tid = cuda.threadIdx.x\n",
    "    bid = cuda.blockIdx.x\n",
    "    bdim = cuda.blockDim.x\n",
    "    gdim = cuda.gridDim.x\n",
    "    \n",
    "    if idx < out.size:\n",
    "        out[idx] = tid * 1000 + bid * 100 + bdim * 10 + gdim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "108aeaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  42. 1042. 2042. 3042.  142. 1142. 2142. 3142.    0.    0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:697: NumbaPerformanceWarning: Grid size 2 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/cudadrv/devicearray.py:937: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "out = np.zeros(10)\n",
    "test_func[2, 4](out)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d_multidim",
   "metadata": {},
   "source": [
    "### Многомерные массивы (2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def matrix_op(matrix):\n",
    "    x, y = cuda.grid(2)\n",
    "    if x < matrix.shape[0] and y < matrix.shape[1]:\n",
    "        matrix[x, y] = cuda.threadIdx.x * 100 + cuda.threadIdx.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d_call",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   1.   2.   3.   0.   1.   2.   3.]\n",
      " [100. 101. 102. 103. 100. 101. 102. 103.]\n",
      " [200. 201. 202. 203. 200. 201. 202. 203.]\n",
      " [300. 301. 302. 303. 300. 301. 302. 303.]\n",
      " [  0.   1.   2.   3.   0.   1.   2.   3.]\n",
      " [100. 101. 102. 103. 100. 101. 102. 103.]\n",
      " [200. 201. 202. 203. 200. 201. 202. 203.]\n",
      " [300. 301. 302. 303. 300. 301. 302. 303.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:697: NumbaPerformanceWarning: Grid size 4 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/cudadrv/devicearray.py:937: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "matrix = np.zeros((8, 8))\n",
    "matrix_op[(2, 2), (4, 4)](matrix)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24253aa2",
   "metadata": {},
   "source": [
    "### Основные функции\n",
    "\n",
    "- `cuda.grid(1)` - получить глобальный индекс потока (1D)\n",
    "- `cuda.grid(2)` - получить глобальные индексы (x, y) для 2D\n",
    "- `cuda.threadIdx.x`, `cuda.threadIdx.y` - индекс потока в блоке\n",
    "- `cuda.blockIdx.x`, `cuda.blockIdx.y` - индекс блока\n",
    "- `cuda.blockDim.x`, `cuda.blockDim.y` - размер блока\n",
    "- `cuda.gridDim.x`, `cuda.gridDim.y` - размер сетки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cuda_shared",
   "metadata": {},
   "source": [
    "## Работа с памятью\n",
    "Еще одно фундаментальное отличие от кода, написанного для выполнения на процессоре -- работа с памятью: нам нужно самим выделять дополнительную память для промежуточных вычислений при работе с GPU, ровно как и загружать данные из памяти компьютера в память GPU.\n",
    "\n",
    "\n",
    "### Shared memory\n",
    "Небольшой блок памяти(64-256кБ) в котором мы можем хранить промежуточные вычисления, которыми могут обмениваться между собой потоки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cuda_shared_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: 1000000.0\n",
      "Expected: 1000000\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def sum_reduce(arr, out):\n",
    "    # Создаем shared memory размером 256 элементов\n",
    "    shared = cuda.shared.array(256, dtype=np.float32)\n",
    "    \n",
    "    # Получаем индексы текущего потока\n",
    "    idx = cuda.grid(1)\n",
    "    tid = cuda.threadIdx.x\n",
    "    \n",
    "    # Копируем данные из глобальной памяти в shared memory\n",
    "    # Пример: если arr = [1,2,3,4], то shared = [1,2,3,4,0,0,...]\n",
    "    if idx < arr.size:\n",
    "        shared[tid] = arr[idx]\n",
    "    else:\n",
    "        shared[tid] = 0\n",
    "    \n",
    "    # Синхронизируем все потоки\n",
    "    # Работает так: останавливает данный поток, и ждёт, пока остальные потоки не вызовут эту функцию\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Выполняем параллельную редукцию\n",
    "    # Пример для массива [1,2,3,4]:\n",
    "    # Шаг 1: [3,2,7,4,0,0,...] (1+2, 3+4)\n",
    "    # Шаг 2: [10,2,7,4,0,0,...] (3+7)\n",
    "    s = 1\n",
    "    while s < cuda.blockDim.x:\n",
    "        if tid % (2 * s) == 0:\n",
    "            shared[tid] += shared[tid + s]\n",
    "        s *= 2\n",
    "        cuda.syncthreads()\n",
    "    \n",
    "    # Первый поток записывает финальный результат\n",
    "    # результат = 10 (сумма всех элементов)\n",
    "    if tid == 0: # смотрим, что это первый элемент\n",
    "        # out[0] = out[0] + shared[0] - выдаёт неправильный результат из-за параллельного выполнения нескольких потоков\n",
    "        cuda.atomic.add(out, 0, shared[0])\n",
    "        \n",
    "        \n",
    "n = 1000000\n",
    "arr = cuda.to_device(np.ones(n, dtype=np.float32))\n",
    "out = cuda.to_device(np.zeros(1, dtype=np.float32))\n",
    "\n",
    "\n",
    "threads = 256\n",
    "blocks = (n + threads - 1) // threads\n",
    "sum_reduce[blocks, threads](arr, out)\n",
    "\n",
    "result = out.copy_to_host()\n",
    "print(f\"Sum: {result[0]}\")\n",
    "print(f\"Expected: {n}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cuda_memory",
   "metadata": {},
   "source": [
    "### Копирование данных CPU -- GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cuda_memory_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:697: NumbaPerformanceWarning: Grid size 4 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def multiply_by_two(arr):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < arr.size:\n",
    "        arr[idx] *= 2\n",
    "\n",
    "\n",
    "x_host = np.ones(1000) # массив в RAM\n",
    "x_device = cuda.to_device(x_host) # перенесли на GPU\n",
    "\n",
    "nulls_device = cuda.device_array(1000) # пустой массив в GPU\n",
    "\n",
    "\n",
    "threadsperblock = 256\n",
    "blockspergrid = (x_device.size + (threadsperblock - 1)) // threadsperblock\n",
    "\n",
    "\n",
    "multiply_by_two[blockspergrid, threadsperblock](x_device)\n",
    "\n",
    "result = x_device.copy_to_host() # переносим обратно в RAM\n",
    "\n",
    "print(result[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3b23a1",
   "metadata": {},
   "source": [
    "Стоит понимать, что функции работают именно с загруженными массивами(numba.cuda.cudadrv.devicearray.DeviceNDArray), и если мы вызываем функцию от обычного np.NDArray:\n",
    "```python\n",
    "arr = np.zeros(5)\n",
    "func[1, 5](arr)\n",
    "```\n",
    "то numba просто рассахаривает код в что-то типа:\n",
    "```python\n",
    "arr = np.zeros(5)\n",
    "gpu_zeros = cuda.to_device(arr)\n",
    "func[1, 5](gpu_zeros)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f466f826",
   "metadata": {},
   "source": [
    "## Потоки\n",
    "Мы можем ещё сильнее углубиться в многопоточность!\n",
    "Фактически, каждый cuda.steam реализует отдельный поток вычислений. Посмотрим на примерах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "176245f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream1 result: [2. 2. 2. 2. 2.]\n",
      "Stream2 result: [4. 4. 4. 4. 4.]\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def kernel(x, y):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < x.size:\n",
    "        y[idx] = x[idx] * 2\n",
    "\n",
    "# Создаём два потока для асинхронного выполнения\n",
    "stream1 = cuda.stream()\n",
    "stream2 = cuda.stream()\n",
    "\n",
    "n = 1000000\n",
    "x1 = cuda.to_device(np.ones(n))\n",
    "y1 = cuda.device_array(n)\n",
    "x2 = cuda.to_device(np.ones(n) * 2)\n",
    "y2 = cuda.device_array(n)\n",
    "\n",
    "# Запускаем в разных потоках параллельно\n",
    "threads = 256\n",
    "blocks = (n + threads - 1) // threads\n",
    "kernel[blocks, threads, stream1](x1, y1)\n",
    "kernel[blocks, threads, stream2](x2, y2)\n",
    "\n",
    "# Синхронизируем оба потока\n",
    "stream1.synchronize()  # Ждём завершения stream1\n",
    "stream2.synchronize()  # Ждём завершения stream2\n",
    "\n",
    "# Копируем результаты обратно\n",
    "result1 = y1.copy_to_host()\n",
    "result2 = y2.copy_to_host()\n",
    "\n",
    "print(\"Stream1 result:\", result1[:5])\n",
    "print(\"Stream2 result:\", result2[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17939244",
   "metadata": {},
   "source": [
    "Стоит выделить один важный случай использования потоков:\n",
    "Если есть огромный массив, то можно разбить его на несколько подмассивов поменьше, каждый из которых мы запустим в отдельном stream. \n",
    "Это позволяет уменьшить задержки на копирование всего массива в память устройства, позволяя производить вычисления, пока какие-то данные еще передаются."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cuda_timing",
   "metadata": {},
   "source": [
    "### Сравнение CPU vs GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae17fc2",
   "metadata": {},
   "source": [
    "Простые операции: сложение векторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9877ab86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU (NumPy):\n",
      "28.2 ms ± 568 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "GPU (CUDA):\n",
      "1.02 ms ± 17.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def add_gpu(x, y, out):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < x.size:\n",
    "        out[idx] = x[idx] + y[idx]\n",
    "\n",
    "n = 10000000\n",
    "x = np.random.rand(n)\n",
    "y = np.random.rand(n)\n",
    "\n",
    "print('CPU (NumPy):')\n",
    "%timeit x + y\n",
    "\n",
    "x_d = cuda.to_device(x)\n",
    "y_d = cuda.to_device(y)\n",
    "out_d = cuda.device_array(n)\n",
    "threads = 256\n",
    "blocks = (n + threads - 1) // threads\n",
    "\n",
    "print('GPU (CUDA):')\n",
    "%timeit add_gpu[blocks, threads](x_d, y_d, out_d); cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035db4ba",
   "metadata": {},
   "source": [
    "Нахождение суммы элементов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc53faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU (NumPy sum):\n",
      "67.9 ms ± 2.24 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "GPU:\n",
      "9.69 ms ± 26.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "n = 100000000\n",
    "arr = np.random.rand(n)\n",
    "\n",
    "print('CPU (NumPy sum):')\n",
    "%timeit np.sum(arr)\n",
    "\n",
    "arr_d = cuda.to_device(arr.astype(np.float32))\n",
    "out_d = cuda.to_device(np.zeros(1, dtype=np.float32))\n",
    "threads = 256\n",
    "blocks = (n + threads - 1) // threads\n",
    "\n",
    "print('GPU:')\n",
    "%timeit sum_reduce[blocks, threads](arr_d, out_d); cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20f5b2e",
   "metadata": {},
   "source": [
    "Поэлементные операции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6e32eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU (NumPy sin):\n",
      "134 ms ± 23.6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "GPU (CUDA sin):\n",
      "1.64 ms ± 25.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def sin_gpu(x, out):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < x.size:\n",
    "        out[idx] = cuda.libdevice.sin(x[idx])\n",
    "\n",
    "n = 10000000\n",
    "x = np.random.rand(n)\n",
    "\n",
    "print('CPU (NumPy sin):')\n",
    "%timeit np.sin(x)\n",
    "\n",
    "x_d = cuda.to_device(x)\n",
    "out_d = cuda.device_array(n)\n",
    "threads = 256\n",
    "blocks = (n + threads - 1) // threads\n",
    "\n",
    "print('GPU (CUDA sin):')\n",
    "%timeit sin_gpu[blocks, threads](x_d, out_d); cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new_example",
   "metadata": {},
   "source": [
    "Умножение матриц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "matmul_example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU (NumPy):\n",
      "1.22 s ± 222 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "GPU (CUDA):\n",
      "52.9 ms ± 18.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def matmul_gpu(A, B, C):\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < C.shape[0] and j < C.shape[1]:\n",
    "        tmp = 0.0\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[i, k] * B[k, j]\n",
    "        C[i, j] = tmp\n",
    "\n",
    "n = 4096\n",
    "A = np.random.rand(n, n).astype(np.float32)\n",
    "B = np.random.rand(n, n).astype(np.float32)\n",
    "\n",
    "print('CPU (NumPy):')\n",
    "%timeit np.dot(A, B)\n",
    "\n",
    "A_d = cuda.to_device(A)\n",
    "B_d = cuda.to_device(B)\n",
    "C_d = cuda.device_array((n, n), dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "print('GPU (CUDA):')\n",
    "%timeit matmul_gpu[(16, 16), (32, 32)](A_d, B_d, C_d); cuda.synchronize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
